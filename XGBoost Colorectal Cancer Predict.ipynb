{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.python.keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tempfile\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "scaler = StandardScaler()\n",
    "oversample = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('../colo_data_mar22_d032222.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "\n",
    "    'age',\n",
    "    'sex',\n",
    "    \n",
    "    'race7',\n",
    "    'educat',\n",
    "    'marital',\n",
    "    'occupat',\n",
    "\n",
    "    'cig_stat',\n",
    "    'cig_years',\n",
    "    'cigpd_f',\n",
    "    'cigar',\n",
    "    'pipe',\n",
    "\n",
    "    'fh_cancer',\n",
    "    'colo_fh',\n",
    "    'colo_fh_cnt',\n",
    "\n",
    "    'bmi_curr',\n",
    "\n",
    "    'asp',\n",
    "    'asppd',\n",
    "    'ibup',\n",
    "    'ibuppd',\n",
    "    \n",
    "    'arthrit_f',\n",
    "    'bronchit_f',\n",
    "    'colon_comorbidity',\n",
    "    'diabetes_f',\n",
    "    'divertic_f',\n",
    "    'emphys_f',\n",
    "    'gallblad_f',\n",
    "    'hearta_f',\n",
    "    'hyperten_f',\n",
    "    'liver_comorbidity',\n",
    "    'osteopor_f',\n",
    "    'polyps_f',\n",
    "    'stroke_f',\n",
    "\n",
    "    'colo_cancer'\n",
    "\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling null values\n",
    "for column in df:\n",
    "\n",
    "    # raw numeric features\n",
    "    if (column == 'age' or column == 'bmi_curr' or column == 'cig_years'): \n",
    "        df[column] = df[column].fillna(df[column].mean())\n",
    "        \n",
    "    # default siblings to 0 (average is 0.11)\n",
    "    elif (column == 'colo_fh_cnt'): \n",
    "        df[column] = df[column].fillna(0)\n",
    "       \n",
    "    # rest are categorized  \n",
    "    else: df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['colo_cancer']\n",
    "X = df.drop('colo_cancer', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = numpy.bincount(y)\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))\n",
    "\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "neg, pos = numpy.bincount(y)\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(train_features, train_labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_decisionTree = xgb.XGBClassifier()\n",
    "XGB_decisionTree = XGB_decisionTree.fit(X=train_features, y=train_labels, eval_set=[(val_features, val_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = XGB_decisionTree.predict(test_features)\n",
    "\n",
    "print(\"Accuracy:\",XGB_decisionTree.score(test_features, test_labels))\n",
    "print()\n",
    "print(metrics.classification_report(test_labels, prediction))\n",
    "\n",
    "# metrics for positive class\n",
    "Precision_pos = metrics.precision_score(test_labels, prediction, pos_label=1)\n",
    "Recall = metrics.recall_score(test_labels, prediction, pos_label=1)\n",
    "F1_score_pos = metrics.f1_score(test_labels, prediction, pos_label=1)\n",
    "\n",
    "#metrics for negative class\n",
    "Precision_neg = metrics.precision_score(test_labels, prediction, pos_label=0)\n",
    "Specificity = metrics.recall_score(test_labels, prediction, pos_label=0)\n",
    "F1_score_neg = metrics.f1_score(test_labels, prediction, pos_label=0)\n",
    "\n",
    "# recall and specificity are opposites \n",
    "\n",
    "auc = metrics.roc_auc_score(test_labels, prediction)\n",
    "prc = metrics.average_precision_score(test_labels, prediction)\n",
    "\n",
    "#metrics:\n",
    "print(\n",
    "    \"Metrics for positive class (most important)\\n\"+\n",
    "    \"Precision: \"+str(Precision_pos)+\n",
    "    \"\\nRecall: \"+str(Recall)+\n",
    "    \"\\nF1_score: \"+str(F1_score_pos)\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\n",
    "    \"Metrics for negative class\\n\"+\n",
    "    \"Precision: \"+str(Precision_neg)+\n",
    "    \"\\nSpecificity: \"+str(Specificity)+\n",
    "    \"\\nF1_score: \"+str(F1_score_neg)\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\n",
    "    \"AUC-ROC: \"+str(auc)+\n",
    "    \"\\nPRC: \"+str(prc)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = metrics.confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('(Good) No Cancer Detected (True Negatives):', cm[0][0])\n",
    "  print('(Okay) No Cancer Thought to Have Cancer (False Positives):', cm[0][1])\n",
    "  print('(Bad) Cancer Thought to Have No Cancer (False Negatives):', cm[1][0])\n",
    "  print('(Good) Cancer Detected (True Positives):', cm[1][1])\n",
    "  print('Total Cancer Cases:', numpy.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(test_labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.RocCurveDisplay.from_predictions(test_labels, prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.PrecisionRecallDisplay.from_predictions(test_labels, prediction)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
